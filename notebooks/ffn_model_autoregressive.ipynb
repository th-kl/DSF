{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Autoregressive Feedforward Neural Network (AR-Net)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "flu = pd.read_csv('../data/INFLUENZA_sentinella/data.csv')\n",
    "weather = pd.read_csv('../data/weather/reg_weather.csv')\n",
    "google_flu = pd.read_csv('../data/google_search_trend/reg_google_grippe.csv')\n",
    "google_symptoms = pd.read_csv('../data/google_search_trend/reg_google_fieber_husten.csv')\n",
    "pop = pd.read_csv('../data/pop_data_cantons/weekly_imputed_pop_data_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Consolidation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Extract relevant data from BAG dataset on weekly flu incidence</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe for regional observations, no differentiation between sex or age\n",
    "flu_reg = flu.query('georegion_type == \"sentinella_region\" and agegroup == \"all\" and sex == \"all\"').copy()\n",
    "\n",
    "# Drop rows for georegion \"unknown\", which only contain NaNs using mask\n",
    "flu_reg = flu_reg[~(flu_reg['georegion'] == 'unknown')]\n",
    "\n",
    "# Select columns required for analysis\n",
    "selected_cols = ['temporal', 'georegion', 'incValue', 'value']\n",
    "flu_reg = flu_reg[selected_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Convert dates and format of Google-Trend data for subsequent merging</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align time-indexes of google data and flu data using created date to iso-calendar week dict\n",
    "with open('date_dict.json', 'r') as f:\n",
    "    date_dict = json.load(f)\n",
    "\n",
    "# Create new column 'Woche' containing iso-calendar weeks for google-trend dates \n",
    "google_flu['Woche'] = google_flu['Woche'].apply(lambda x: date_dict[x]) \n",
    "google_symptoms['Woche'] = google_symptoms['Woche'].apply(lambda x: date_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape google_flu from wide to long to enable merging on date and region \n",
    "google_flu = google_flu.melt(id_vars=['Woche'], var_name='region_query', value_name='search_activity')\n",
    "\n",
    "# Separate region and query information from header into separate rows\n",
    "google_flu['region'] = google_flu['region_query'].apply(lambda x: \"_\".join(x.split('_')[:2]))\n",
    "google_flu['query'] = google_flu['region_query'].apply(lambda x: \"_\".join(x.split('_')[2:]))\n",
    "google_flu.drop(columns='region_query', inplace=True) # Drop superfluous region_query column\n",
    "\n",
    "# Reshape dataframe to get separate columns for each variable\n",
    "google_flu = google_flu.pivot(index=['Woche', 'region'], columns='query', values='search_activity').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Repeat above process for second google dataset containing data on symptom queries\n",
    "# Reshape google_flu from wide to long\n",
    "google_symptoms = google_symptoms.melt(id_vars=['Woche'], var_name='region_query', value_name='search_activity')\n",
    "\n",
    "# Separate region and query information from header into separate rows\n",
    "google_symptoms['region'] = google_symptoms['region_query'].apply(lambda x: \"_\".join(x.split('_')[:2]))\n",
    "google_symptoms['query'] = google_symptoms['region_query'].apply(lambda x: \"_\".join(x.split('_')[2:]))\n",
    "google_symptoms.drop(columns='region_query', inplace=True) # Drop superfluous region_query column\n",
    "\n",
    "# Reshape dataframe to get separate columns for each variable\n",
    "google_symptoms = google_symptoms.pivot(index=['Woche', 'region'], columns='query', values='search_activity').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Convert date format of weather data for merging</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dates to 'YYYY-Www' ISO week format\n",
    "dates = weather.date.values\n",
    "iso_week_dates = [datetime.strptime(date, '%Y-%m-%d').isocalendar()[:2] for date in dates]\n",
    "iso_week_dates = [f'{year}-W{week:02d}' for year, week in iso_week_dates]\n",
    "weather['date'] = iso_week_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Merge datasets on date and region</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge(flu_reg, weather, how='left', left_on=['temporal', 'georegion'], right_on=['date', 'region']).sort_values(by=['georegion', 'temporal'])\n",
    "merged_google = pd.merge(google_flu, google_symptoms, how='inner', on=['region', 'Woche'])\n",
    "merged_data = pd.merge(merged_data, merged_google, how='left', left_on=['georegion', 'temporal'], right_on=['region', 'Woche'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.drop(columns=['region_x', 'region_y', 'date', 'Woche'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Convert ISO calendar-weeks to Gregorian calendar (format 'YYYY-MM-DD')</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Convert ISO calendar-weeks to gregorian dates\n",
    "# Functions based on answer by Ben James: <https://stackoverflow.com/questions/304256/whats-the-best-way-to-find-the-inverse-of-datetime-isocalendar>\n",
    "def iso_year_start(iso_year):\n",
    "    \"The gregorian calendar date of the first day of the given ISO year\"\n",
    "    fourth_jan = datetime.date(iso_year, 1, 4)\n",
    "    delta = datetime.timedelta(fourth_jan.isoweekday()-1)\n",
    "    return fourth_jan - delta \n",
    "\n",
    "def iso_to_gregorian(iso_year, iso_week, iso_day):\n",
    "    \"Gregorian calendar date for the given ISO year, week and day\"\n",
    "    year_start = iso_year_start(iso_year)\n",
    "    return year_start + datetime.timedelta(days=iso_day-1, weeks=iso_week-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract week number and year from date column in ISO calendar week \n",
    "week_pattern = r'W(\\d{1,2})' # RegEx pattern to extract week nr without trailing zero\n",
    "merged_data['week_number'] = merged_data['temporal'].str.extract(week_pattern).astype(int)\n",
    "merged_data['year'] = merged_data['temporal'].apply(lambda x: x.split('-')[0])\n",
    "merged_data['year'] = pd.to_numeric(merged_data['year']) # Convert from string to numeric\n",
    "\n",
    "# Convert from iso-calendar week to gregorian dates (format: YYYY-MM-DD)\n",
    "merged_data['date'] = list(map(lambda year, week: iso_to_gregorian(year, week, 4), merged_data['year'], merged_data['week_number']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect resulting dataframe\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data exploration</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display descriptive statistics\n",
    "merged_data['incValue'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display missing values in reported flu incidence across regions\n",
    "merged_data[merged_data['incValue'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values in March 2020 linearly\n",
    "merged_data['incValue'].interpolate(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Inspect incidence of consultations for influenza-like-diseases over time</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for cantons within each region\n",
    "region_to_ct = {'region_1': ['Genf', 'Neuenburg', 'Waadt', 'Wallis'], \n",
    "           'region_2': ['Bern', 'Freiburg', 'Jura'], \n",
    "           'region_3': ['Aargau', 'Basel-Landschaft', 'Basel-Stadt', 'Solothurn'], \n",
    "           'region_4': ['Luzern', 'Nidwalden', 'Obwalden', 'Schwyz', 'Uri', 'Zug'], \n",
    "           'region_5': ['Appenzell_Innerrhoden', 'Appenzell_Ausserrhoden', 'Glarus', 'Sankt_Gallen', 'Schaffhausen', 'Thurgau', 'Zürich'], \n",
    "           'region_6': ['Graubünden', 'Tessin']}\n",
    "\n",
    "# Plot the regional incidence value for the provided timeframe\n",
    "\n",
    "#  Set plot style\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Create subplots for each region\n",
    "fig, ax = plt.subplots(6, figsize=(10, 15))  # Adjusted figure size for better spacing\n",
    "fig.suptitle('Weekly incidence of consultations for influenza-like-diseases per region from 2013-2023', fontsize=12)\n",
    "\n",
    "# Adjust the spacing of the subplots\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "fig.subplots_adjust(hspace=0.5)  # Adjust horizontal space between plots\n",
    "\n",
    "for i in range(1, 7):\n",
    "    ax[i-1].axvline(pd.Timestamp('2020-01-01'), linestyle='--', color='grey', lw=1, alpha=.7)\n",
    "    ax[i-1].plot(merged_data.set_index('date').loc[merged_data.set_index('date')['georegion'] == f\"region_{i}\", 'incValue'])\n",
    "    ax[i-1].set_title(f\"Region {i}: \\n{region_to_ct[f'region_{i}']}\", fontsize=10)\n",
    "\n",
    "    if i == 5:\n",
    "    # Change the color and line width of the spines for region 5\n",
    "        for spine in ax[i-1].spines.values():\n",
    "            spine.set_edgecolor('black')\n",
    "            spine.set_linewidth(2)\n",
    "            spine.set_visible(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Plot distribution of incidence for Region 5</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_data = merged_data.loc[\n",
    "    (merged_data['georegion'] == \"region_5\") & \n",
    "    (merged_data['date'].apply(lambda x: x.year < 2020)),\n",
    "    ['date', 'incValue']\n",
    "    ].copy()\n",
    "\n",
    "histogram_data.set_index('date', inplace=True)\n",
    "histogram_data.hist()\n",
    "\n",
    "# Adding a title to the plot\n",
    "plt.title('Histogram of Incidence (\"incValue\") in Region 5 for Timeframe from 2013 to 2020', fontsize=12)\n",
    "plt.xlabel('Incidence')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Power Transforms\n",
    "\n",
    "\n",
    "The above plot shows that the data is strongly skewed due to the long periods of low incidence and sudden sharp peaks during flu-season. As will be demonstrated later in this notebook, using a PowerTransform to scale the data helped the models capture the movement in the data much better. \n",
    "\n",
    "The plots below demonstrate the effect of applying the Yeo-Johnson transform on the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Create the PowerTransformer with 'yeo-johnson' method\n",
    "power_transformer = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "# Fit and transform the filtered data\n",
    "transformed_data = power_transformer.fit_transform(histogram_data.values.reshape(-1, 1))\n",
    "\n",
    "# Create a figure and axis for the transformed data\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(histogram_data.index, transformed_data)\n",
    "ax.set_title('Region 5 incValue, Yeo-Johnson Transform', fontsize=10)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.hist(transformed_data, bins=30, alpha=0.5)\n",
    "ax.set_title('Histogram of Transformed \"incValue\"', fontsize=10)\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Modelling: AR-NET</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Set-Up</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the autoregressive nature of this network, the following functions are required:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* create_lagged_features: Outputs a dataframe with columns at a specified range of lags given an input variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lagged_features(df, column, number_of_lags, seasonal_lags=None):\n",
    "    # Copy the original DataFrame to avoid modifying it\n",
    "    df_lagged = df.copy()\n",
    "\n",
    "    # Generate regular lagged features\n",
    "    for lag in range(1, number_of_lags + 1):\n",
    "        df_lagged[f'lag_{lag}'] = df_lagged[column].shift(lag)\n",
    "\n",
    "    # Generate seasonal lags\n",
    "    if seasonal_lags is not None:\n",
    "        for season_lag in seasonal_lags:\n",
    "            df_lagged[f'seasonal_lag_{season_lag}_helper'] = df_lagged[column].shift(season_lag-1)\n",
    "            df_lagged[f'seasonal_lag_{season_lag}'] = df_lagged[column].shift(season_lag)\n",
    "\n",
    "    return df_lagged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* autoregressive_iterative_forecast: Perform iterative one-step-ahead forecasting using the autoregressive model for a specified number of steps given initial input of lag observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoregressive_iterative_forecast(model, initial_input, seasonal_input, n_steps):\n",
    "    \"\"\"\n",
    "    Perform iterative forecasting using an autoregressive model.\n",
    "\n",
    "    Args:\n",
    "        model: Trained autoregressive model (MLPRegressor).\n",
    "        initial_input: The initial input features (the first observation from the test set).\n",
    "        n_steps: Number of future time steps to forecast.\n",
    "\n",
    "    Returns:\n",
    "        A list of forecasts, one for each future time step.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    current_input = initial_input.copy()\n",
    "    current_input = np.array(current_input)\n",
    "    seasonal_input = np.array(seasonal_input)\n",
    "    forecasts = []\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        # Predict the next step\n",
    "        next_step_pred = model.predict(current_input.reshape(1, -1))[0] \n",
    "        forecasts.append(next_step_pred)\n",
    "        \n",
    "        # Update the current input to include the new prediction by rolling all lags except the last one (seasonal lag)\n",
    "        current_input[:-1] = np.roll(current_input[:-1], 1)\n",
    "        current_input[0] = next_step_pred # Update the most recent lag with the prediction from the week before\n",
    "        \n",
    "        # Update the seasonal lag (52-period lag)\n",
    "        if i < 52:\n",
    "            # Use the actual seasonal lag value for the first 52 weeks\n",
    "            current_input[-1] = seasonal_input[i]\n",
    "        else:\n",
    "            # Use forecasted value for the seasonal lag after 52 weeks\n",
    "            current_input[-1] = forecasts[i - 52]\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return np.array(forecasts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model Tuning</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training approach initially involved fitting the model on the training set within each fold of a five-fold timeseries-split cross-validation. In each fold, the model would predict the length of the validation set iteratively, using the initial validation set predictors. Direct multi-step methods exist but were not considered due to time constraints. However, we noticed high variability in performance on the test set when following the cross validation results. Three factors seemed to be at play:\n",
    "\n",
    "1. First, the models were overfitting on the initial ranges of hyperparameters: \n",
    "- Architecture: Ranging between 1 and 4 layers in depth and 32 to 100 neurons per layer \n",
    "\n",
    "- L2 Regularization: Ranging between 0.001 and 0.1\n",
    "\n",
    "Overfitting showed itself when plotting the training and validation loss for each split. While the training loss decreased quickly and steadily, the validation loss appeared noisy. In further testing, narrower and shallower networks proved to generalize much better, capturing the movement of the seasons much more closely in terms of timing and scale. Secondly, higher levels of regularization, between 0.01 and 0.3, helped with regulating the number of autoregressive lags the model considered. Whereas the best-performing models in validation were using close to a year of lags in addition to the seasonal lag at 52 weeks, much fewer lags were used under the applied changes. \n",
    "\n",
    "2. The initial learning rate for the adam solver was set too high at 0.01, preventing some architectures from learning altogether. A range between 10e-3 and 10e-4 has been found to produce good results for our purposes.\n",
    "\n",
    "These modifications already significantly stabilized cross-validation results in terms of trend capture over the longer term and generalization on the test set. However, the cross validation results remained somewhat unreliable with too large of a share of the top performing models not generalizing well, producing overblown predictions and not capturing trends on the training timeframe either. \n",
    "\n",
    "3. This was found to be driven by the number of validation folds. Upon inspection of the predictions on the validation splits, it became evident that earlier folds were not getting enough training data due to the limited size of our overall training set (spanning 364 weeks of data between 2013 and 2020). Using multiple folds for cross-validation therefore produced noise by including models trained on insufficient data for an adequate predictions. By contrast, a simple train-validation split on the training data provided much more reliable results. \n",
    "\n",
    "This is also apparent from the following code. The subchapter \"K-fold Cross Validation\" hereafter, demonstrates the aforementioned problem of unreliable cross validation results although it already only employs a two-fold expanding window cross validation approach. The 5 best-performing models from cross validation are selected and used for predictions on the test set. Please refer to the plots at the end of the chapter for further discussion of the results and the subsequent re-estimation using a simple training-validation split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>K-fold Cross Validation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Define train-test split</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for region 5 from 2013 to 2020 \n",
    "data = merged_data.loc[(merged_data['georegion'] == \"region_5\") & (merged_data['date'].apply(lambda x: x.year < 2020))]\n",
    "\n",
    "# Split the data into training and test set\n",
    "y = data['incValue']\n",
    "split = int(len(y) * 0.8)\n",
    "y_train, y_test = y[:split], y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Suppress convergence warnings\n",
    "# warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "\n",
    "# Define parameter configurations to assess\n",
    "lags = 52 # Autoregressive lags to consider\n",
    "hidden_layer_sizes = [(16, 16), (16, 16, 16), (32, 32)]\n",
    "alphas = np.linspace(0.01, 0.3, num=100) # Regularization parameter\n",
    "batch_size = 32\n",
    "learning_rates = np.logspace(-3, -4, 100)\n",
    "activations = ['relu']\n",
    "seasonal = [52]\n",
    "\n",
    "\n",
    "# Extract data\n",
    "data = merged_data.loc[(merged_data['georegion'] == \"region_5\") & (merged_data['date'].apply(lambda x: x.year < 2020))]\n",
    "# Split the data\n",
    "y = data['incValue']\n",
    "split = int(len(y) * 0.8)\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "\n",
    "# Keep track of configurations and cv scores\n",
    "k_fold_scores_df = pd.DataFrame(columns=['RMSE', 'lags', 'seasonal_lags', 'hidden_layers', 'alpha', 'batch_size', 'activation', 'learning_rate'])\n",
    "i = 0 # Initialize counters for assigning validation scores to scores_df \n",
    "\n",
    "# Randomized search for hyperparameter configurations\n",
    "random.seed(42)\n",
    "iterations = 100\n",
    "\n",
    "# Plotting of CV performance - Plots predictions on train and validation set within each fold; please also uncomment the section below and the highlighted section at the end of the loop \n",
    "rows = math.ceil(3 * iterations / 4)\n",
    "fig, axs = plt.subplots(rows, 4, figsize=(20, 6*rows)) # Adjust figsize\n",
    "axs = axs.flatten()\n",
    "k = 0 # Counter for plotting of validation prediction within folds\n",
    "\n",
    "# Random search loop for hyperparameter configurations\n",
    "for _ in range(iterations):\n",
    "    # Randomly select hyperparameters\n",
    "    lag = random.randint(15, lags-10)\n",
    "    activation = random.choice(activations)\n",
    "    learning_rate = random.choice(learning_rates)\n",
    "    alpha = random.choice(alphas)\n",
    "    hidden_layer_size = random.choice(hidden_layer_sizes)\n",
    "\n",
    "    model = MLPRegressor(max_iter=1000, \n",
    "                        random_state=42, \n",
    "                        solver='adam', \n",
    "                        activation=activation, \n",
    "                        hidden_layer_sizes=hidden_layer_size, \n",
    "                        alpha=alpha, \n",
    "                        batch_size=batch_size, \n",
    "                        learning_rate_init=learning_rate,\n",
    "                        warm_start=False, \n",
    "                        early_stopping=True)\n",
    "    scores = []\n",
    "    tscv = TimeSeriesSplit(n_splits=3, test_size=52)\n",
    "    fold = 0\n",
    "    \n",
    "    # Create lagged features based on the whole y_train\n",
    "    df_lagged = create_lagged_features(pd.DataFrame(y_train, columns=['incValue']), column='incValue', number_of_lags=lag, seasonal_lags=seasonal)\n",
    "    df_lagged.dropna(inplace=True)\n",
    "    \n",
    "    training_cols = [col for col in df_lagged.columns if ('lag_' in col) and ('_helper' not in col)]\n",
    "    X = df_lagged[training_cols]\n",
    "    X_seasonal = df_lagged['seasonal_lag_52_helper']\n",
    "    y = df_lagged['incValue']\n",
    "    \n",
    "    for train_index, val_index in tscv.split(X):\n",
    "        \n",
    "        y_train_cv, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        X_train_cv, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        X_seasonal_train, X_seasonal_val = X_seasonal.iloc[train_index], X_seasonal.iloc[val_index]\n",
    "\n",
    "        # Take the first row of X_train_cv (the oldest lags)\n",
    "        oldest_lags = X_train_cv.iloc[0, 1:].values.reshape(1, -1)\n",
    "\n",
    "        # Concatenate y_train_cv with the oldest lags\n",
    "        combined_data = np.vstack((y_train_cv.values.reshape(-1, 1), oldest_lags.T))\n",
    "\n",
    "        # Fit the PowerTransformer and StandardScaler on the available lags in the training data (incl. lags in first row of lag df_train)\n",
    "        pt = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "        stdscaler = StandardScaler()\n",
    "        combined_data_transformed = pt.fit_transform(combined_data)\n",
    "        stdscaler.fit(combined_data_transformed)\n",
    "        \n",
    "        # Apply Transform to the entire y_train_cv\n",
    "        y_train_cv_transformed = pt.transform(y_train_cv.values.reshape(-1, 1)).flatten()\n",
    "        y_val_transformed = pt.transform(y_val.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "        # Apply the PowerTransformer to each lagged feature in X_train_cv and X_val\n",
    "        X_train_cv_transformed = X_train_cv.apply(lambda column: pt.transform(column.values.reshape(-1, 1)).flatten())\n",
    "        X_val_transformed = X_val.apply(lambda column: pt.transform(column.values.reshape(-1, 1)).flatten())\n",
    "        X_seasonal_train_trans = pt.transform(X_seasonal_train.values.reshape(-1, 1)).flatten()\n",
    "        X_seasonal_val_trans = pt.transform(X_seasonal_val.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "        \n",
    "        # Apply StandardScaler()\n",
    "        y_train_cv_scaled = stdscaler.transform(y_train_cv_transformed.reshape(-1, 1)).flatten()\n",
    "        y_val_scaled = stdscaler.transform(y_val_transformed.reshape(-1, 1)).flatten()\n",
    "        X_train_cv_scaled = X_train_cv_transformed.apply(lambda column: stdscaler.transform(column.values.reshape(-1, 1)).flatten())\n",
    "        X_val_scaled = X_val_transformed.apply(lambda column: stdscaler.transform(column.values.reshape(-1, 1)).flatten())\n",
    "        X_seasonal_train_scaled = stdscaler.transform(X_seasonal_train_trans.reshape(-1, 1)).flatten()\n",
    "        X_seasonal_val_scaled = stdscaler.transform(X_seasonal_val_trans.reshape(-1, 1)).flatten()\n",
    "\n",
    "        ######################\n",
    "        # NOTE: PLOT VALIDATION AND TRAINING LOSSES - Adjust max_iter to 1, set warm_start = True to enable, uncomment 'ConvergenceWarning'-filter\n",
    "\n",
    "        # training_losses = []\n",
    "        # validation_losses = []\n",
    "\n",
    "        # for epoch in range(1000):  # Adjust the number of epochs as needed\n",
    "        #     model.fit(X_train_cv_scaled.values, y_train_cv_scaled)\n",
    "\n",
    "        #     # Store training loss from the last iteration\n",
    "        #     training_losses.append(model.loss_curve_[-1])\n",
    "\n",
    "        #     # Compute and store validation loss\n",
    "        #     val_predictions = model.predict(X_val_scaled.values)\n",
    "        #     val_loss = mean_squared_error(y_val_scaled, val_predictions)\n",
    "        #     validation_losses.append(val_loss)\n",
    "        \n",
    "        # if fold == 2:\n",
    "        #     plt.plot(training_losses, label='Training Loss')\n",
    "        #     # If you have validation loss, plot it here\n",
    "        #     plt.plot(validation_losses, label='Validation Loss')\n",
    "\n",
    "        #     plt.title('Learning Curve')\n",
    "        #     plt.xlabel('Epochs')\n",
    "        #     plt.ylabel('Loss')\n",
    "        #     plt.title(f'Lags: {lag}, Learning-rate: {learning_rate}, alpha: {alpha}, hidden layers: {hidden_layer_size}')\n",
    "        #     plt.legend()\n",
    "        #     plt.show()\n",
    "\n",
    "        #######################\n",
    "\n",
    "        # Fit model\n",
    "        model.fit(X_train_cv_scaled.values, y_train_cv_scaled)\n",
    "        # loss_values = model.loss_curve_\n",
    "        \n",
    "        # Make iterative forecasts (NOTE: train and val splits are numpy arrays, seasonal helper columns necessary for updating of seasonal lag)\n",
    "        # print(f'X_val_scaled: {X_val_scaled}')\n",
    "        # print(f'X_val_scaled: {X_val_scaled.iloc[0]}')\n",
    "        prediction = autoregressive_iterative_forecast(model, X_val_scaled.iloc[0], X_seasonal_val_scaled, len(y_val_scaled))\n",
    "        y_hat_train = autoregressive_iterative_forecast(model, X_train_cv_scaled.iloc[0], X_seasonal_train_scaled, len(y_train_cv_scaled))\n",
    "        prediction = np.array(prediction).flatten()\n",
    "        y_hat_train = np.array(y_hat_train).flatten()\n",
    "        rmse = mean_squared_error(y_val_scaled, prediction, squared=False)\n",
    "\n",
    "        # Reverse transformt to plot original scale train-validation results\n",
    "        prediction = stdscaler.inverse_transform(prediction.reshape(-1, 1))\n",
    "        y_hat_train = stdscaler.inverse_transform(y_hat_train.reshape(-1, 1))\n",
    "        prediction = pt.inverse_transform(prediction.reshape(-1, 1))\n",
    "        y_hat_train = pt.inverse_transform(y_hat_train.reshape(-1, 1)) \n",
    "        rmse = mean_squared_error(y_val, prediction, squared=False)\n",
    "\n",
    "        scores.append(rmse)\n",
    "        #######################\n",
    "        # NOTE: UNCOMMENT FOR PLOTS OF VALIDATION - Plot actual vs predicted values\n",
    "\n",
    "        ## PLOT SCALED PREDICTIONS\n",
    "        # ax = axs[k]\n",
    "        # ax.plot(range(len(y_train_cv_scaled)), y_train_cv_scaled, label='Training Actual', color='blue')\n",
    "        # ax.plot(range(len(y_train_cv_scaled), len(y_train_cv_scaled) + len(y_val_scaled)), y_val_scaled, label='Validation Actual', color='blue')\n",
    "        # ax.plot(range(len(y_train_cv_scaled), len(y_train_cv_scaled) + len(y_val_scaled)), prediction, label='Validation Predicted', color='red', linestyle='--')\n",
    "        # ax.plot(range(len(y_train_cv_scaled)), y_hat_train, label='Train-Set Prediction', color='orange', linestyle='--')\n",
    "        \n",
    "        ## PLOT ORIGINAL SCALE\n",
    "        ax = axs[k]\n",
    "        ax.plot(range(len(y_train_cv)), y_train_cv, label='Training Actual', color='blue')\n",
    "        ax.plot(range(len(y_train_cv), len(y_train_cv) + len(y_val)), y_val, label='Validation Actual', color='blue')\n",
    "        ax.plot(range(len(y_train_cv), len(y_train_cv) + len(y_val)), prediction, label='Validation Predicted', color='red', linestyle='--')\n",
    "        ax.plot(range(len(y_train_cv)), y_hat_train, label='Train-Set Prediction', color='orange', linestyle='--')\n",
    "        \n",
    "        ax.set_title(f'Nr: {i}; Lag: {lag}; alpha: {alpha:.2f}; hidden layers: {hidden_layer_size}', fontsize=10)\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel('Scaled Value')\n",
    "        ax.legend()\n",
    "        k += 1\n",
    "        ########################\n",
    "        fold += 1\n",
    "    \n",
    "    # Fill in parameters and score for each configuration \n",
    "    k_fold_scores_df.loc[i, 'lags'] = lag\n",
    "    k_fold_scores_df.loc[i, 'seasonal_lags'] = seasonal\n",
    "    k_fold_scores_df.loc[i, 'hidden_layers'] = hidden_layer_size\n",
    "    k_fold_scores_df.loc[i, 'alpha'] = alpha\n",
    "    k_fold_scores_df.loc[i, 'batch_size'] = batch_size\n",
    "    k_fold_scores_df.loc[i, 'activation'] = activation\n",
    "    k_fold_scores_df.loc[i, 'learning_rate'] = learning_rate\n",
    "    k_fold_scores_df.loc[i, 'RMSE'] = np.mean(scores)\n",
    "    print(f'{i}/{iterations}: {(i/iterations)*100:.2f}%')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold_scores_df['RMSE'] = pd.to_numeric(k_fold_scores_df['RMSE'])\n",
    "# Best parameters and score\n",
    "best_config_index = k_fold_scores_df['RMSE'].idxmin()  # This gets the index of the minimum RMSE\n",
    "best_config = k_fold_scores_df.loc[best_config_index]  # Use the index to access the row\n",
    "best_score = best_config['RMSE']\n",
    "print(f\"Best parameters: {best_config}\")\n",
    "print(f\"Best score (RMSE): {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold_scores_df.sort_values(by='RMSE').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Assessing the 5 best-performing models on the test set</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_nr = 1\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(20, 15))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for index_nr in k_fold_scores_df.sort_values(by='RMSE').head(5).index:\n",
    "\n",
    "    best_config = k_fold_scores_df.loc[index_nr]\n",
    "\n",
    "    best_lag = best_config.values[1]\n",
    "    best_seasonal_lag = best_config.values[2]\n",
    "    best_hidden_layers = best_config.values[3]\n",
    "    best_alpha = best_config.values[4]\n",
    "    best_batch_size = best_config.values[5]\n",
    "    best_activation = best_config.values[6]\n",
    "    best_learning_rate = best_config.values[7]\n",
    "\n",
    "    # Extract data\n",
    "    data = merged_data.loc[(merged_data['georegion'] == \"region_5\") & (merged_data['date'].apply(lambda x: x.year < 2020))]\n",
    "    # Split the data\n",
    "    y = data['incValue']\n",
    "\n",
    "    # Create lagged features based on the whole y\n",
    "    df_lagged = create_lagged_features(pd.DataFrame(y, columns=['incValue']), column='incValue', number_of_lags=best_lag, seasonal_lags=best_seasonal_lag)\n",
    "\n",
    "    split = int(len(y) * 0.8)\n",
    "    # NOTE: SPLIT BEFORE DROPPING TO AVOID DATA LEAKAGE\n",
    "    df_lagged_train = df_lagged.iloc[:split]\n",
    "    df_lagged_train = df_lagged_train.dropna()\n",
    "    df_lagged_test = df_lagged.iloc[split:]\n",
    "\n",
    "    # Extract training columns and output variable from dataframe\n",
    "    training_cols = [col for col in df_lagged.columns if ('lag_' in col) and ('helper' not in col)]\n",
    "    X_train = df_lagged_train[training_cols]\n",
    "    y_train = df_lagged_train['incValue']\n",
    "    X_test = df_lagged_test[training_cols]\n",
    "    y_test = df_lagged_test['incValue']\n",
    "\n",
    "    # Columns required for rolling of seasonal lag in iterative autoregressive forecast\n",
    "    X_train_seasonal = df_lagged_train['seasonal_lag_52_helper']\n",
    "    X_test_seasonal = df_lagged_test['seasonal_lag_52_helper']\n",
    "\n",
    "    # Create combined data to fit transform on all available historical lags in training set\n",
    "    oldest_lags = X_train.iloc[0, 1:].values.reshape(1, -1) # Take the first row of X_train_cv (the oldest lags)\n",
    "    combined_data = np.vstack((y_train.values.reshape(-1, 1), oldest_lags.T)) # Concatenate y_train_cv with the oldest lags\n",
    "\n",
    "    # Fit Yeo-Johnson Transform on combined data\n",
    "    pt = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "    stdscaler = StandardScaler()\n",
    "    combined_data_transformed = pt.fit_transform(combined_data)\n",
    "    stdscaler.fit(combined_data_transformed)\n",
    "\n",
    "    # Apply transform and scaling to train and test sets\n",
    "    y_train_transformed = pt.transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "    y_test_transformed = pt.transform(y_test.values.reshape(-1, 1)).flatten()\n",
    "    X_train_transformed = X_train.apply(lambda x: pt.transform(x.values.reshape(-1, 1)).flatten())\n",
    "    X_test_transformed = X_test.apply(lambda x: pt.transform(x.values.reshape(-1, 1)).flatten())\n",
    "    # X_train_transformed = X_train.apply(lambda x: pt.transform(x))\n",
    "    # X_test_transformed = X_test.apply(lambda x: pt.transform(x.values.reshape(-1, 1))).flatten()\n",
    "    X_train_seasonal_trans = pt.transform(X_train_seasonal.values.reshape(-1, 1)).flatten()\n",
    "    X_test_seasonal_trans = pt.transform(X_test_seasonal.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Apply StandardScaler\n",
    "    y_train_scaled = stdscaler.transform(y_train_transformed.reshape(-1, 1)).flatten()\n",
    "    y_test_scaled = stdscaler.transform(y_test_transformed.reshape(-1, 1)).flatten()\n",
    "    X_train_scaled = X_train_transformed.apply(lambda x: stdscaler.transform(x.values.reshape(-1, 1)).flatten())\n",
    "    X_test_scaled = X_test_transformed.apply(lambda x: stdscaler.transform(x.values.reshape(-1, 1)).flatten())\n",
    "    X_train_seasonal_scaled = stdscaler.transform(X_train_seasonal_trans.reshape(-1, 1)).flatten()\n",
    "    X_test_seasonal_scaled = stdscaler.transform(X_test_seasonal_trans.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Initialize the final model configuration\n",
    "    final_model = MLPRegressor(max_iter=1000, \n",
    "                        random_state=42, \n",
    "                        solver='adam', \n",
    "                        activation=best_activation, \n",
    "                        hidden_layer_sizes=(best_hidden_layers), \n",
    "                        alpha=best_alpha, \n",
    "                        batch_size=best_batch_size, \n",
    "                        learning_rate_init=best_learning_rate)\n",
    "\n",
    "    # Train final model\n",
    "    final_model.fit(X_train_scaled.values, y_train_scaled) \n",
    "\n",
    "    # Forecast for the length of the test set\n",
    "    forecasts = autoregressive_iterative_forecast(final_model, X_test_scaled.iloc[0], X_test_seasonal_scaled,len(y_test_scaled))\n",
    "    y_hat_train = autoregressive_iterative_forecast(final_model, X_train_scaled.iloc[0], X_train_seasonal_scaled, len(y_train))\n",
    "\n",
    "    forecasts = stdscaler.inverse_transform(forecasts.reshape(-1, 1))\n",
    "    forecasts = pt.inverse_transform(forecasts.reshape(-1, 1))\n",
    "    y_hat_train = stdscaler.inverse_transform(y_hat_train.reshape(-1, 1))\n",
    "    y_hat_train = pt.inverse_transform(y_hat_train.reshape(-1, 1))\n",
    "\n",
    "    # Evaluate the forecasts against the actual y_test values\n",
    "    rmse = mean_squared_error(y_test, forecasts, squared=False)\n",
    "\n",
    "    print(f\"The RMSE for the forecasts is: {rmse:.3f}\")\n",
    "\n",
    "    # Plot the results\n",
    "    ax = axs[(rank_nr-1)]\n",
    "    ax.plot(df_lagged['incValue'], label=\"True Train\", alpha=1, color='lightblue')\n",
    "    ax.plot(y_test.index, y_test, label=\"True Test\", alpha=0.8, color='blue')\n",
    "    ax.plot(y_test.index, forecasts, label='Predictions', alpha=0.7, color='red', linestyle='--')\n",
    "    ax.plot(y_train.index, y_hat_train, label='Prediction on Train', alpha=0.7, color='grey', linestyle='--')\n",
    "\n",
    "\n",
    "    # Add labels and legend\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Incidence\")\n",
    "    ax.set_title(f'Nr: {rank_nr}, RMSE: {rmse}, lag: {best_lag}, hidden layers: {best_hidden_layers}, alpha: {best_alpha:.4f}, learning rate: {best_learning_rate:.6f}', fontsize=10)\n",
    "    ax.legend()\n",
    "\n",
    "    rank_nr += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The above results from a three-fold time-series split show that the RMSE scores from the cross validation don't quite reflect the model performance on the test set:\n",
    "\n",
    "- Notably, Models Nr. 3 and Nr. 4 show very close RMSEs to Models Nr. 2 and Nr. 5 on the validation set but are not actually capturing the movement of the training data very well. \n",
    "\n",
    "- When plotting the predictions on the training and validation data within each fold during cross validation, the variability of the models throughout the folds becomes evident, even though the seasons remain relatively stable throughout the folds.\n",
    "\n",
    "In order to select the best performing model for application on the test set, it may therefore make sense to only use a single train-validation split on the test set to assess the best performing model without interference of scores based on insufficient data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Simple Training-Validation Split</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following demonstrates the final approach to our autoregressive model which is carried out with a simple training validation split of the training data. \n",
    "\n",
    "Note: \n",
    "- The autoregressive lags are already confined to 23 lags, which, in combination with 15 lags was performing well in previous wide search ranges. We ended up choosing 23 lags (although 15 produced a lower RMSE on the test set) because the produced models generally appeared to capture the onset, duration and scale of the seasons very well even over longer periods when compared with other ranges of lags. \n",
    "\n",
    "- The hidden layers are also already confined to (16, 16) for the same reason. When compared with wider and deeper networks, this architecture appeared to generalize the best and captures the peaks. However, it fails to capture more refined patterns, producing smooth and narrow peaks. \n",
    "\n",
    "- ReLu turned out to produce the best results throughout our testing and has been stacked up against tanh and sigmoid, both of which generally did not capture the patterns well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for region 5 from 2013 to 2020 \n",
    "data = merged_data.loc[(merged_data['georegion'] == \"region_5\") & (merged_data['date'].apply(lambda x: x.year < 2020))]\n",
    "\n",
    "# Split the data into training and test set\n",
    "y = data['incValue']\n",
    "split = int(len(y) * 0.8)\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# Suppress convergence warnings\n",
    "# warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "\n",
    "# Define parameter configurations to assess\n",
    "lags = [23] # Autoregressive lags to consider\n",
    "hidden_layer_sizes = [(16, 16)]\n",
    "alphas = np.linspace(0.01, 0.3, num=100) # Regularization parameter\n",
    "batch_size = 32\n",
    "learning_rates = np.logspace(-3, -4, 100)\n",
    "activations = ['relu']\n",
    "seasonal = [52]\n",
    "\n",
    "\n",
    "# Extract data\n",
    "data = merged_data.loc[(merged_data['georegion'] == \"region_5\") & (merged_data['date'].apply(lambda x: x.year < 2020))]\n",
    "# Split the data\n",
    "y = data['incValue']\n",
    "split = int(len(y) * 0.8)\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "i = 0\n",
    "scores_df = pd.DataFrame(columns=['RMSE', 'lags', 'seasonal_lags', 'hidden_layers', 'alpha', 'batch_size', 'activation', 'learning_rate'])\n",
    "\n",
    "# Grid search hyperparameter configurations\n",
    "random.seed(42)\n",
    "iterations = 500\n",
    "\n",
    "for _ in range(iterations):\n",
    "    # Randomly select hyperparameters\n",
    "    lag = random.choice(lags)\n",
    "    activation = random.choice(activations)\n",
    "    learning_rate = random.choice(learning_rates)\n",
    "    alpha = random.choice(alphas)\n",
    "    hidden_layer_size = random.choice(hidden_layer_sizes)\n",
    "\n",
    "    # Keep track of configurations and cv scores\n",
    "    model = MLPRegressor(max_iter=1000, \n",
    "                        random_state=42, \n",
    "                        solver='adam', \n",
    "                        activation=activation, \n",
    "                        hidden_layer_sizes=hidden_layer_size, \n",
    "                        alpha=alpha, \n",
    "                        batch_size=batch_size, \n",
    "                        learning_rate_init=learning_rate,\n",
    "                        warm_start=False, \n",
    "                        early_stopping=True)\n",
    "    scores = []\n",
    "    tscv = TimeSeriesSplit(n_splits=2, test_size=52)\n",
    "    fold = 0\n",
    "    \n",
    "    # Create lagged features based on the whole y_train\n",
    "    df_lagged = create_lagged_features(pd.DataFrame(y_train, columns=['incValue']), column='incValue', number_of_lags=lag, seasonal_lags=seasonal)\n",
    "    df_lagged.dropna(inplace=True)\n",
    "    \n",
    "    training_cols = [col for col in df_lagged.columns if ('lag_' in col) and ('_helper' not in col)]\n",
    "    X = df_lagged[training_cols]\n",
    "    X_seasonal = df_lagged['seasonal_lag_52_helper']\n",
    "    y = df_lagged['incValue']\n",
    "\n",
    "    val_index = range(len(y) - 52, len(y))\n",
    "    train_index = range(0, len(y) - 52)\n",
    "        \n",
    "    y_train_cv, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    X_train_cv, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    X_seasonal_train, X_seasonal_val = X_seasonal.iloc[train_index], X_seasonal.iloc[val_index]\n",
    "\n",
    "    # Take the first row of X_train_cv (the oldest lags)\n",
    "    oldest_lags = X_train_cv.iloc[0, 1:].values.reshape(1, -1)\n",
    "\n",
    "    # Concatenate y_train_cv with the oldest lags\n",
    "    combined_data = np.vstack((y_train_cv.values.reshape(-1, 1), oldest_lags.T))\n",
    "\n",
    "    # Fit the PowerTransformer and StandardScaler on the available lags in the training data (incl. lags in first row of lag df_train)\n",
    "    pt = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "    stdscaler = StandardScaler()\n",
    "    combined_data_transformed = pt.fit_transform(combined_data)\n",
    "    stdscaler.fit(combined_data_transformed)\n",
    "    \n",
    "    # Apply Transform to the entire y_train_cv\n",
    "    y_train_cv_transformed = pt.transform(y_train_cv.values.reshape(-1, 1)).flatten()\n",
    "    y_val_transformed = pt.transform(y_val.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Apply the PowerTransformer to each lagged feature in X_train_cv and X_val\n",
    "    X_train_cv_transformed = X_train_cv.apply(lambda column: pt.transform(column.values.reshape(-1, 1)).flatten())\n",
    "    X_val_transformed = X_val.apply(lambda column: pt.transform(column.values.reshape(-1, 1)).flatten())\n",
    "    X_seasonal_train_trans = pt.transform(X_seasonal_train.values.reshape(-1, 1)).flatten()\n",
    "    X_seasonal_val_trans = pt.transform(X_seasonal_val.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "    \n",
    "    # Apply StandardScaler()\n",
    "    y_train_cv_scaled = stdscaler.transform(y_train_cv_transformed.reshape(-1, 1)).flatten()\n",
    "    y_val_scaled = stdscaler.transform(y_val_transformed.reshape(-1, 1)).flatten()\n",
    "    X_train_cv_scaled = X_train_cv_transformed.apply(lambda column: stdscaler.transform(column.values.reshape(-1, 1)).flatten())\n",
    "    X_val_scaled = X_val_transformed.apply(lambda column: stdscaler.transform(column.values.reshape(-1, 1)).flatten())\n",
    "    X_seasonal_train_scaled = stdscaler.transform(X_seasonal_train_trans.reshape(-1, 1)).flatten()\n",
    "    X_seasonal_val_scaled = stdscaler.transform(X_seasonal_val_trans.reshape(-1, 1)).flatten()\n",
    "\n",
    "    ######################\n",
    "    # NOTE: PLOT VALIDATION AND TRAINING LOSSES - Adjust max_iter to 1, set warm_start = True to enable, uncomment 'ConvergenceWarning'-filter\n",
    "\n",
    "    # training_losses = []\n",
    "    # validation_losses = []\n",
    "\n",
    "    # for epoch in range(1000):  # Adjust the number of epochs as needed\n",
    "    #     model.fit(X_train_cv_scaled.values, y_train_cv_scaled)\n",
    "\n",
    "    #     # Store training loss from the last iteration\n",
    "    #     training_losses.append(model.loss_curve_[-1])\n",
    "\n",
    "    #     # Compute and store validation loss\n",
    "    #     val_predictions = model.predict(X_val_scaled.values)\n",
    "    #     val_loss = mean_squared_error(y_val_scaled, val_predictions)\n",
    "    #     validation_losses.append(val_loss)\n",
    "    \n",
    "    # if fold == 2:\n",
    "    #     plt.plot(training_losses, label='Training Loss')\n",
    "    #     # If you have validation loss, plot it here\n",
    "    #     plt.plot(validation_losses, label='Validation Loss')\n",
    "\n",
    "    #     plt.title('Learning Curve')\n",
    "    #     plt.xlabel('Epochs')\n",
    "    #     plt.ylabel('Loss')\n",
    "    #     plt.title(f'Lags: {lag}, Learning-rate: {learning_rate}, alpha: {alpha}, hidden layers: {hidden_layer_size}')\n",
    "    #     plt.legend()\n",
    "    #     plt.show()\n",
    "\n",
    "    #######################\n",
    "\n",
    "    # Fit model\n",
    "    model.fit(X_train_cv_scaled.values, y_train_cv_scaled)\n",
    "    # loss_values = model.loss_curve_\n",
    "    \n",
    "    # Make iterative forecasts (NOTE: train and val splits are numpy arrays, seasonal helper columns necessary for updating of seasonal lag)\n",
    "    # print(f'X_val_scaled: {X_val_scaled}')\n",
    "    # print(f'X_val_scaled: {X_val_scaled.iloc[0]}')\n",
    "    prediction = autoregressive_iterative_forecast(model, X_val_scaled.iloc[0], X_seasonal_val_scaled, len(y_val_scaled))\n",
    "    y_hat_train = autoregressive_iterative_forecast(model, X_train_cv_scaled.iloc[0], X_seasonal_train_scaled, len(y_train_cv_scaled))\n",
    "    prediction = np.array(prediction).flatten()\n",
    "    y_hat_train = np.array(y_hat_train).flatten()\n",
    "    rmse = mean_squared_error(y_val_scaled, prediction, squared=False)\n",
    "\n",
    "    # NOTE: UNCOMMENT FOR ORIGINAL SCALE PLOTTING AND RMSE - Reverse transform to plot original scale train-validation results\n",
    "    prediction = stdscaler.inverse_transform(prediction.reshape(-1, 1))\n",
    "    y_hat_train = stdscaler.inverse_transform(y_hat_train.reshape(-1, 1))\n",
    "    prediction = pt.inverse_transform(prediction.reshape(-1, 1))\n",
    "    y_hat_train = pt.inverse_transform(y_hat_train.reshape(-1, 1)) \n",
    "    rmse = mean_squared_error(y_val, prediction, squared=False)\n",
    "\n",
    "    scores.append(rmse)\n",
    "    #######################\n",
    "    # NOTE: UNCOMMENT FOR PLOTS OF VALIDATION - Plot actual vs predicted values\n",
    "    # plt.figure(figsize=(5, 3))\n",
    "\n",
    "    ## PLOT SCALED PREDICTIONS - Ensure to comment out the inverse scaling just above\n",
    "    # plt.plot(range(len(y_train_cv_scaled)), y_train_cv_scaled, label='Training Actual', color='blue')\n",
    "    # plt.plot(range(len(y_train_cv_scaled), len(y_train_cv_scaled) + len(y_val_scaled)), y_val_scaled, label='Validation Actual', color='blue')\n",
    "    # plt.plot(range(len(y_train_cv_scaled), len(y_train_cv_scaled) + len(y_val_scaled)), prediction, label='Validation Predicted', color='red', linestyle='--')\n",
    "    # plt.plot(range(len(y_train_cv_scaled)), y_hat_train, label='Train-Set Prediction', color='orange', linestyle='--')\n",
    "    \n",
    "    ## PLOT ORIGINAL SCALE\n",
    "    # plt.plot(range(len(y_train_cv)), y_train_cv, label='Training Actual', color='blue')\n",
    "    # plt.plot(range(len(y_train_cv), len(y_train_cv) + len(y_val)), y_val, label='Validation Actual', color='blue')\n",
    "    # plt.plot(range(len(y_train_cv), len(y_train_cv) + len(y_val)), prediction, label='Validation Predicted', color='red', linestyle='--')\n",
    "    # plt.plot(range(len(y_train_cv)), y_hat_train, label='Train-Set Prediction', color='orange', linestyle='--')\n",
    "    \n",
    "    # plt.title(f'Nr: {i}; Lag: {lag}; Fold {fold+1}; alpha: {alpha}; hidden layers: {hidden_layer_size}', fontsize=10)\n",
    "    # plt.xlabel('Time')\n",
    "    # plt.ylabel('Scaled Value')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "    ########################\n",
    "    \n",
    "    # Fill in parameters and score for each configuration \n",
    "    scores_df.loc[i, 'lags'] = lag\n",
    "    scores_df.loc[i, 'seasonal_lags'] = seasonal\n",
    "    scores_df.loc[i, 'hidden_layers'] = hidden_layer_size\n",
    "    scores_df.loc[i, 'alpha'] = alpha\n",
    "    scores_df.loc[i, 'batch_size'] = batch_size\n",
    "    scores_df.loc[i, 'activation'] = activation\n",
    "    scores_df.loc[i, 'learning_rate'] = learning_rate\n",
    "    scores_df.loc[i, 'RMSE'] = np.mean(scores)\n",
    "    print(f'{i}/{iterations}: {(i/iterations)*100:.2f}%')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df['RMSE'] = pd.to_numeric(scores_df['RMSE'])\n",
    "# Best parameters and score\n",
    "best_config_index = scores_df['RMSE'].idxmin()  # This gets the index of the minimum RMSE\n",
    "best_config = scores_df.loc[best_config_index]  # Use the index to access the row\n",
    "best_score = best_config['RMSE']\n",
    "print(f\"Best parameters: {best_config}\")\n",
    "print(f\"Best score (RMSE): {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.sort_values(by='RMSE').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_nr = 1\n",
    "\n",
    "fig, axs = plt.subplots(5, figsize=(10, 25))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for index_nr in scores_df.sort_values(by='RMSE').head(5).index:\n",
    "    best_config = scores_df.loc[index_nr]\n",
    "\n",
    "    best_lag = best_config.values[1]\n",
    "    best_seasonal_lag = best_config.values[2]\n",
    "    best_hidden_layers = best_config.values[3]\n",
    "    best_alpha = best_config.values[4]\n",
    "    best_batch_size = best_config.values[5]\n",
    "    best_activation = best_config.values[6]\n",
    "    best_learning_rate = best_config.values[7]\n",
    "\n",
    "    # Extract data\n",
    "    data = merged_data.loc[(merged_data['georegion'] == \"region_5\") & (merged_data['date'].apply(lambda x: x.year < 2020))]\n",
    "    # Split the data\n",
    "    y = data['incValue']\n",
    "\n",
    "    # Create lagged features based on the whole y\n",
    "    df_lagged = create_lagged_features(pd.DataFrame(y, columns=['incValue']), column='incValue', number_of_lags=best_lag, seasonal_lags=best_seasonal_lag)\n",
    "\n",
    "    split = int(len(y) * 0.8)\n",
    "    # NOTE: SPLIT BEFORE DROPPING TO AVOID DATA LEAKAGE\n",
    "    df_lagged_train = df_lagged.iloc[:split]\n",
    "    df_lagged_train = df_lagged_train.dropna()\n",
    "    df_lagged_test = df_lagged.iloc[split:]\n",
    "\n",
    "    # Extract training columns and output variable from dataframe\n",
    "    training_cols = [col for col in df_lagged.columns if ('lag_' in col) and ('helper' not in col)]\n",
    "    X_train = df_lagged_train[training_cols]\n",
    "    y_train = df_lagged_train['incValue']\n",
    "    X_test = df_lagged_test[training_cols]\n",
    "    y_test = df_lagged_test['incValue']\n",
    "\n",
    "    # Columns required for rolling of seasonal lag in iterative autoregressive forecast\n",
    "    X_train_seasonal = df_lagged_train['seasonal_lag_52_helper']\n",
    "    X_test_seasonal = df_lagged_test['seasonal_lag_52_helper']\n",
    "\n",
    "    # Create combined data to fit transform on all available historical lags in training set\n",
    "    oldest_lags = X_train.iloc[0, 1:].values.reshape(1, -1) # Take the first row of X_train_cv (the oldest lags)\n",
    "    combined_data = np.vstack((y_train.values.reshape(-1, 1), oldest_lags.T)) # Concatenate y_train_cv with the oldest lags\n",
    "\n",
    "    # Fit Yeo-Johnson Transform on combined data\n",
    "    pt = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "    stdscaler = StandardScaler()\n",
    "    combined_data_transformed = pt.fit_transform(combined_data)\n",
    "    stdscaler.fit(combined_data_transformed)\n",
    "\n",
    "    # Apply transform and scaling to train and test sets\n",
    "    y_train_transformed = pt.transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "    y_test_transformed = pt.transform(y_test.values.reshape(-1, 1)).flatten()\n",
    "    X_train_transformed = X_train.apply(lambda x: pt.transform(x.values.reshape(-1, 1)).flatten())\n",
    "    X_test_transformed = X_test.apply(lambda x: pt.transform(x.values.reshape(-1, 1)).flatten())\n",
    "    # X_train_transformed = X_train.apply(lambda x: pt.transform(x))\n",
    "    # X_test_transformed = X_test.apply(lambda x: pt.transform(x.values.reshape(-1, 1))).flatten()\n",
    "    X_train_seasonal_trans = pt.transform(X_train_seasonal.values.reshape(-1, 1)).flatten()\n",
    "    X_test_seasonal_trans = pt.transform(X_test_seasonal.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Apply StandardScaler\n",
    "    y_train_scaled = stdscaler.transform(y_train_transformed.reshape(-1, 1)).flatten()\n",
    "    y_test_scaled = stdscaler.transform(y_test_transformed.reshape(-1, 1)).flatten()\n",
    "    X_train_scaled = X_train_transformed.apply(lambda x: stdscaler.transform(x.values.reshape(-1, 1)).flatten())\n",
    "    X_test_scaled = X_test_transformed.apply(lambda x: stdscaler.transform(x.values.reshape(-1, 1)).flatten())\n",
    "    X_train_seasonal_scaled = stdscaler.transform(X_train_seasonal_trans.reshape(-1, 1)).flatten()\n",
    "    X_test_seasonal_scaled = stdscaler.transform(X_test_seasonal_trans.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Initialize the final model configuration\n",
    "    final_model = MLPRegressor(max_iter=1000, \n",
    "                        random_state=42, \n",
    "                        solver='adam', \n",
    "                        activation=best_activation, \n",
    "                        hidden_layer_sizes=(best_hidden_layers), \n",
    "                        alpha=best_alpha, \n",
    "                        batch_size=best_batch_size, \n",
    "                        learning_rate_init=best_learning_rate)\n",
    "\n",
    "    # Train final model\n",
    "    final_model.fit(X_train_scaled.values, y_train_scaled) \n",
    "\n",
    "    # Forecast for the length of the test set\n",
    "    forecasts = autoregressive_iterative_forecast(final_model, X_test_scaled.iloc[0], X_test_seasonal_scaled,len(y_test_scaled))\n",
    "    y_hat_train = autoregressive_iterative_forecast(final_model, X_train_scaled.iloc[0], X_train_seasonal_scaled, len(y_train))\n",
    "\n",
    "    # Reverse transformations\n",
    "    forecasts = stdscaler.inverse_transform(forecasts.reshape(-1, 1))\n",
    "    forecasts = pt.inverse_transform(forecasts.reshape(-1, 1))\n",
    "    y_hat_train = stdscaler.inverse_transform(y_hat_train.reshape(-1, 1))\n",
    "    y_hat_train = pt.inverse_transform(y_hat_train.reshape(-1, 1))\n",
    "\n",
    "    # Evaluate the forecasts against the actual y_test values\n",
    "    rmse = mean_squared_error(y_test, forecasts, squared=False)\n",
    "\n",
    "    # Plot the predictions\n",
    "    ax = axs[(rank_nr-1)]\n",
    "    ax.plot(df_lagged['incValue'], label=\"True Train\", alpha=1, color='lightblue')\n",
    "    ax.plot(y_test.index, y_test, label=\"True Test\", alpha=0.8, color='blue')\n",
    "    ax.plot(y_test.index, forecasts, label='Predictions', alpha=0.7, color='red', linestyle='--')\n",
    "    ax.plot(y_train.index, y_hat_train, label='Prediction on Train', alpha=0.7, color='grey', linestyle='--')\n",
    "\n",
    "    # Add labels and legend\n",
    "    ax.set_title(f'Nr: {rank_nr}, RMSE: {rmse}, lag: {best_lag}, hidden layers: {best_hidden_layers}, alpha: {best_alpha:.4f}, learning rate: {best_learning_rate:.6f}', fontsize=10)\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Incidence\")\n",
    "    ax.legend()\n",
    "\n",
    "    rank_nr += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Simple Train-Validation split without Yeo Johnson Transform</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the following demonstrates the performance of the model on the above specifications without applying a PowerTransform to make the data more normally distributed. We generally found that the models weren't able to capture the movement in the data well.\n",
    "\n",
    "The code below tests a range of lags on the hyperparameter configuration from before only using the MinMaxScaler. The plots at the end demonstrate the difficulty the models have at capturing the seasonal movement well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "data = merged_data.loc[(merged_data['georegion'] == \"region_5\") & (merged_data['date'].apply(lambda x: x.year < 2020))]\n",
    "# Split the data\n",
    "y = data['incValue']\n",
    "split = int(len(y) * 0.8)\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "i = 0\n",
    "scale_scores_df = pd.DataFrame(columns=['RMSE', 'lags', 'seasonal_lags', 'hidden_layers', 'alpha', 'batch_size', 'activation', 'learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify scaler to apply to the code below (MinMaxScaler or StandardScaler)\n",
    "specify_scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Define parameter configurations to assess\n",
    "lags = 52 # Autoregressive lags to consider\n",
    "hidden_layer_sizes = [(16, 16)]\n",
    "alphas = np.linspace(0.01, 0.3, num=100) # Regularization parameter\n",
    "batch_size = 32\n",
    "learning_rates = np.logspace(-3, -4, 100)\n",
    "activations = ['relu']\n",
    "seasonal = [52]\n",
    "\n",
    "\n",
    "# Extract data\n",
    "data = merged_data.loc[(merged_data['georegion'] == \"region_5\") & (merged_data['date'].apply(lambda x: x.year < 2020))]\n",
    "# Split the data\n",
    "y = data['incValue']\n",
    "split = int(len(y) * 0.8)\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "i = 0\n",
    "scores_df = pd.DataFrame(columns=['RMSE', 'lags', 'seasonal_lags', 'hidden_layers', 'alpha', 'batch_size', 'activation', 'learning_rate'])\n",
    "\n",
    "# Randomized search of hyperparameter configurations\n",
    "random.seed(42)\n",
    "iterations = 100\n",
    "\n",
    "for _ in range(iterations):\n",
    "    # Randomly select hyperparameters\n",
    "    lag = random.randint(15, lags-10)\n",
    "    activation = random.choice(activations)\n",
    "    learning_rate = random.choice(learning_rates)\n",
    "    alpha = random.choice(alphas)\n",
    "    hidden_layer_size = random.choice(hidden_layer_sizes)\n",
    "\n",
    "    # Keep track of configurations and cv scores\n",
    "    model = MLPRegressor(max_iter=2000, \n",
    "                        random_state=42, \n",
    "                        solver='adam', \n",
    "                        activation=activation, \n",
    "                        hidden_layer_sizes=hidden_layer_size, \n",
    "                        alpha=alpha, \n",
    "                        batch_size=batch_size, \n",
    "                        learning_rate_init=learning_rate,\n",
    "                        warm_start=False, \n",
    "                        early_stopping=True)\n",
    "    scores = []\n",
    "    \n",
    "    # Create lagged features based on the whole y_train\n",
    "    df_lagged = create_lagged_features(pd.DataFrame(y_train, columns=['incValue']), column='incValue', number_of_lags=lag, seasonal_lags=seasonal)\n",
    "    df_lagged.dropna(inplace=True)\n",
    "    \n",
    "    training_cols = [col for col in df_lagged.columns if ('lag_' in col) and ('_helper' not in col)]\n",
    "    X = df_lagged[training_cols]\n",
    "    X_seasonal = df_lagged['seasonal_lag_52_helper']\n",
    "    y = df_lagged['incValue']\n",
    "    # print(X)\n",
    "    # print(y)\n",
    "    \n",
    "    val_index = range(len(y) - 52, len(y))\n",
    "    train_index = range(0, len(y) - 52)\n",
    "        \n",
    "    y_train_cv, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    X_train_cv, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    X_seasonal_train, X_seasonal_val = X_seasonal.iloc[train_index], X_seasonal.iloc[val_index]\n",
    "\n",
    "    # Take the first row of X_train_cv (the oldest lags)\n",
    "    oldest_lags = X_train_cv.iloc[0, 1:].values.reshape(1, -1)\n",
    "\n",
    "    # Concatenate y_train_cv with the oldest lags\n",
    "    combined_data = np.vstack((y_train_cv.values.reshape(-1, 1), oldest_lags.T))\n",
    "\n",
    "    # Fit the scaler on the available lags in the training data (incl. lags in first row of lag df_train)\n",
    "    scaler = specify_scaler # Specify scaler above\n",
    "    scaler.fit(combined_data)\n",
    "    \n",
    "    # Apply Transform to the entire y_train_cv\n",
    "    y_train_cv_scaled = scaler.transform(y_train_cv.values.reshape(-1, 1)).flatten()\n",
    "    y_val_scaled = scaler.transform(y_val.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Apply the PowerTransformer to each lagged feature in X_train_cv and X_val\n",
    "    X_train_cv_scaled = X_train_cv.apply(lambda column: scaler.transform(column.values.reshape(-1, 1)).flatten())\n",
    "    X_val_scaled = X_val.apply(lambda column: scaler.transform(column.values.reshape(-1, 1)).flatten())\n",
    "    X_seasonal_train_scaled = scaler.transform(X_seasonal_train.values.reshape(-1, 1)).flatten()\n",
    "    X_seasonal_val_scaled = scaler.transform(X_seasonal_val.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "    ######################\n",
    "    # NOTE: PLOT VALIDATION AND TRAINING LOSSES - Adjust max_iter to 1 and set warm_start = True to enable\n",
    "\n",
    "    # training_losses = []\n",
    "    # validation_losses = []\n",
    "\n",
    "    # for epoch in range(1000):  # Adjust the number of epochs as needed\n",
    "    #     model.fit(X_train_cv_scaled.values, y_train_cv_scaled)\n",
    "\n",
    "    #     # Store training loss from the last iteration\n",
    "    #     training_losses.append(model.loss_curve_[-1])\n",
    "\n",
    "    #     # Compute and store validation loss\n",
    "    #     val_predictions = model.predict(X_val_scaled.values)\n",
    "    #     val_loss = mean_squared_error(y_val_scaled, val_predictions)\n",
    "    #     validation_losses.append(val_loss)\n",
    "    \n",
    "    # if fold == 2:\n",
    "    #     plt.plot(training_losses, label='Training Loss')\n",
    "    #     # If you have validation loss, plot it here\n",
    "    #     plt.plot(validation_losses, label='Validation Loss')\n",
    "\n",
    "    #     plt.title('Learning Curve')\n",
    "    #     plt.xlabel('Epochs')\n",
    "    #     plt.ylabel('Loss')\n",
    "    #     plt.title(f'Lags: {lag}, Learning-rate: {learning_rate}, alpha: {alpha}, hidden layers: {hidden_layer_size}')\n",
    "    #     plt.legend()\n",
    "    #     plt.show()\n",
    "\n",
    "    #######################\n",
    "\n",
    "    # Fit model\n",
    "    model.fit(X_train_cv_scaled.values, y_train_cv_scaled)\n",
    "    # loss_values = model.loss_curve_\n",
    "    \n",
    "    # Make iterative forecasts (NOTE: train and val splits are numpy arrays, seasonal helper columns necessary for updating of seasonal lag)\n",
    "    # print(f'X_val_scaled: {X_val_scaled}')\n",
    "    # print(f'X_val_scaled: {X_val_scaled.iloc[0]}')\n",
    "    prediction = autoregressive_iterative_forecast(model, X_val_scaled.iloc[0], X_seasonal_val_scaled, len(y_val_scaled))\n",
    "    y_hat_train = autoregressive_iterative_forecast(model, X_train_cv_scaled.iloc[0], X_seasonal_train_scaled, len(y_train_cv_scaled))\n",
    "    prediction = np.array(prediction).flatten()\n",
    "    y_hat_train = np.array(y_hat_train).flatten()\n",
    "\n",
    "    rmse = mean_squared_error(y_val_scaled, prediction, squared=False)\n",
    "\n",
    "    # NOTE: UNCOMMENT FOR ORIGINAL SCALE PLOTTING AND RMSE - Reverse transform to plot original scale train-validation results\n",
    "    prediction = scaler.inverse_transform(prediction.reshape(-1, 1))\n",
    "    y_hat_train = scaler.inverse_transform(y_hat_train.reshape(-1, 1)) \n",
    "    \n",
    "    rmse = mean_squared_error(y_val, prediction, squared=False)\n",
    "\n",
    "    scores.append(rmse)\n",
    "    \n",
    "    ## PLOT SCALED PREDICTIONS - Ensure to comment out the inverse scaling just above\n",
    "    # plt.plot(range(len(y_train_cv_scaled)), y_train_cv_scaled, label='Training Actual', color='blue')\n",
    "    # plt.plot(range(len(y_train_cv_scaled), len(y_train_cv_scaled) + len(y_val_scaled)), y_val_scaled, label='Validation Actual', color='blue')\n",
    "    # plt.plot(range(len(y_train_cv_scaled), len(y_train_cv_scaled) + len(y_val_scaled)), prediction, label='Validation Predicted', color='red', linestyle='--')\n",
    "    # plt.plot(range(len(y_train_cv_scaled)), y_hat_train, label='Train-Set Prediction', color='orange', linestyle='--')\n",
    "    \n",
    "    ## PLOT ORIGINAL SCALE\n",
    "    plt.plot(range(len(y_train_cv)), y_train_cv, label='Training Actual', color='blue')\n",
    "    plt.plot(range(len(y_train_cv), len(y_train_cv) + len(y_val)), y_val, label='Validation Actual', color='blue')\n",
    "    plt.plot(range(len(y_train_cv), len(y_train_cv) + len(y_val)), prediction, label='Validation Predicted', color='red', linestyle='--')\n",
    "    plt.plot(range(len(y_train_cv)), y_hat_train, label='Train-Set Prediction', color='orange', linestyle='--')\n",
    "    \n",
    "    plt.title(f'Nr: {i}; Lag: {lag}; alpha: {alpha}; hidden layers: {hidden_layer_size}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Scaled Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    ########################\n",
    "\n",
    "    \n",
    "    # Fill in parameters and score for each configuration \n",
    "    scale_scores_df.loc[i, 'lags'] = lag\n",
    "    scale_scores_df.loc[i, 'seasonal_lags'] = seasonal\n",
    "    scale_scores_df.at[i, 'hidden_layers'] = hidden_layer_size\n",
    "    scale_scores_df.loc[i, 'alpha'] = alpha\n",
    "    scale_scores_df.loc[i, 'batch_size'] = batch_size\n",
    "    scale_scores_df.loc[i, 'activation'] = activation\n",
    "    scale_scores_df.loc[i, 'learning_rate'] = learning_rate\n",
    "    scale_scores_df.loc[i, 'RMSE'] = np.mean(scores)\n",
    "    print(f'{i}/{iterations}: {(i/iterations)*100:.2f}%')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_scores_df['RMSE'] = pd.to_numeric(scale_scores_df['RMSE'])\n",
    "# Best parameters and score\n",
    "best_config_index = scale_scores_df['RMSE'].idxmin()  # This gets the index of the minimum RMSE\n",
    "best_config = scale_scores_df.loc[best_config_index]  # Use the index to access the row\n",
    "best_score = best_config['RMSE']\n",
    "print(f\"Best parameters: {best_config}\")\n",
    "print(f\"Best score (RMSE): {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_scores_df.sort_values(by='RMSE').head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rank_nr = 1\n",
    "fig, axs = plt.subplots(3, 2, figsize=(20, 15))\n",
    "axs = axs.flatten()\n",
    "for index_nr in scale_scores_df.sort_values(by='RMSE').head(5).index:\n",
    "    best_config = scale_scores_df.loc[index_nr]\n",
    "\n",
    "    best_lag = best_config.values[1]\n",
    "    best_seasonal_lag = best_config.values[2]\n",
    "    best_hidden_layers = best_config.values[3]\n",
    "    best_alpha = best_config.values[4]\n",
    "    best_batch_size = best_config.values[5]\n",
    "    best_activation = best_config.values[6]\n",
    "    best_learning_rate = best_config.values[7]\n",
    "\n",
    "    # Extract data\n",
    "    data = merged_data.loc[(merged_data['georegion'] == \"region_5\") & (merged_data['date'].apply(lambda x: x.year < 2020))]\n",
    "    # Split the data\n",
    "    y = data['incValue']\n",
    "\n",
    "    # Create lagged features based on the whole y\n",
    "    df_lagged = create_lagged_features(pd.DataFrame(y, columns=['incValue']), column='incValue', number_of_lags=best_lag, seasonal_lags=best_seasonal_lag)\n",
    "\n",
    "    split = int(len(y) * 0.8)\n",
    "    # NOTE: SPLIT BEFORE DROPPING TO AVOID DATA LEAKAGE\n",
    "    df_lagged_train = df_lagged.iloc[:split]\n",
    "    df_lagged_train = df_lagged_train.dropna()\n",
    "    df_lagged_test = df_lagged.iloc[split:]\n",
    "\n",
    "    # Extract training columns and output variable from dataframe\n",
    "    training_cols = [col for col in df_lagged.columns if ('lag_' in col) and ('helper' not in col)]\n",
    "    X_train = df_lagged_train[training_cols]\n",
    "    y_train = df_lagged_train['incValue']\n",
    "    X_test = df_lagged_test[training_cols]\n",
    "    y_test = df_lagged_test['incValue']\n",
    "\n",
    "    # Columns required for rolling of seasonal lag in iterative autoregressive forecast\n",
    "    X_train_seasonal = df_lagged_train['seasonal_lag_52_helper']\n",
    "    X_test_seasonal = df_lagged_test['seasonal_lag_52_helper']\n",
    "\n",
    "    # Create combined data to fit transform on all available historical lags in training set\n",
    "    oldest_lags = X_train.iloc[0, 1:].values.reshape(1, -1) # Take the first row of X_train_cv (the oldest lags)\n",
    "    combined_data = np.vstack((y_train.values.reshape(-1, 1), oldest_lags.T)) # Concatenate y_train_cv with the oldest lags\n",
    "\n",
    "    # Fit Yeo-Johnson Transform on combined data\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(combined_data)\n",
    "\n",
    "    # Apply transform and scaling to train and test sets\n",
    "    y_train_scaled = scaler.transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "    y_test_scaled = scaler.transform(y_test.values.reshape(-1, 1)).flatten()\n",
    "    X_train_scaled = X_train.apply(lambda x: scaler.transform(x.values.reshape(-1, 1)).flatten())\n",
    "    X_test_scaled = X_test.apply(lambda x: scaler.transform(x.values.reshape(-1, 1)).flatten())\n",
    "    # X_train_transformed = X_train.apply(lambda x: pt.transform(x))\n",
    "    # X_test_transformed = X_test.apply(lambda x: pt.transform(x.values.reshape(-1, 1))).flatten()\n",
    "    X_train_seasonal_scaled = scaler.transform(X_train_seasonal.values.reshape(-1, 1)).flatten()\n",
    "    X_test_seasonal_scaled = scaler.transform(X_test_seasonal.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Initialize the final model configuration\n",
    "    final_model = MLPRegressor(max_iter=1000, \n",
    "                        random_state=42, \n",
    "                        solver='adam', \n",
    "                        activation=best_activation, \n",
    "                        hidden_layer_sizes=(best_hidden_layers), \n",
    "                        alpha=best_alpha, \n",
    "                        batch_size=best_batch_size, \n",
    "                        learning_rate_init=best_learning_rate)\n",
    "\n",
    "    # Train final model\n",
    "    final_model.fit(X_train_scaled.values, y_train_scaled) \n",
    "\n",
    "    # Forecast for the length of the test set\n",
    "    forecasts = autoregressive_iterative_forecast(final_model, X_test_scaled.iloc[0], X_test_seasonal_scaled,len(y_test_scaled))\n",
    "    y_hat_train = autoregressive_iterative_forecast(final_model, X_train_scaled.iloc[0], X_train_seasonal_scaled, len(y_train))\n",
    "\n",
    "    forecasts = scaler.inverse_transform(forecasts.reshape(-1, 1))\n",
    "    y_hat_train = scaler.inverse_transform(y_hat_train.reshape(-1, 1))\n",
    "\n",
    "    # Evaluate the forecasts against the actual y_test values\n",
    "    rmse = mean_squared_error(y_test, forecasts, squared=False)\n",
    "\n",
    "    # Plot the results\n",
    "    ax = axs[(rank_nr-1)]\n",
    "    ax.plot(df_lagged['incValue'], label=\"True Train\", alpha=1, color='lightblue')\n",
    "    ax.plot(y_test.index, y_test, label=\"True Test\", alpha=0.8, color='blue')\n",
    "    ax.plot(y_test.index, forecasts, label='Predictions', alpha=0.7, color='red', linestyle='--')\n",
    "    ax.plot(y_train.index, y_hat_train, label='Prediction on Train', alpha=0.7, color='grey', linestyle='--')\n",
    "\n",
    "\n",
    "    # Add labels and legend\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Incidence\")\n",
    "    ax.legend()\n",
    "    rank_nr += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
